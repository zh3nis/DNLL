{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0802afa3",
   "metadata": {},
   "source": [
    "# CIFAR-10 Classification with an LDA Head\n",
    "This notebook trains a lightweight convolutional encoder with an LDA head on CIFAR-10, alongside a softmax baseline, then visualises the learned embedding spaces side by side."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c1fc5d",
   "metadata": {},
   "source": [
    "### Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbf11f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from dnll import LDAHead, DNLLLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab011bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device =', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7881674",
   "metadata": {},
   "source": [
    "### Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a8e436e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:13<00:00, 12.9MB/s] \n",
      "/venv/main/lib/python3.12/site-packages/torchvision/datasets/cifar.py:83: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)\n",
      "  entry = pickle.load(f, encoding=\"latin1\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50000, 10000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = (0.4914, 0.4822, 0.4465)\n",
    "std = (0.2470, 0.2435, 0.2616)\n",
    "pin_memory = torch.cuda.is_available()\n",
    "\n",
    "train_tfm = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "test_tfm = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "train_ds = datasets.CIFAR10(root='./data', train=True, transform=train_tfm, download=True)\n",
    "test_ds  = datasets.CIFAR10(root='./data', train=False, transform=test_tfm, download=True)\n",
    "train_ld = DataLoader(train_ds, batch_size=256, shuffle=True, num_workers=4, pin_memory=pin_memory)\n",
    "test_ld  = DataLoader(test_ds,  batch_size=1024, shuffle=False, num_workers=4, pin_memory=pin_memory)\n",
    "len(train_ds), len(test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0efcdea",
   "metadata": {},
   "source": [
    "### Model: encoder + heads (LDA + softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daebb93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "        )\n",
    "        self.proj = nn.Linear(256, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.proj(x)\n",
    "\n",
    "class DeepLDA(nn.Module):\n",
    "    def __init__(self, C, D):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(D)\n",
    "        self.head = LDAHead(C, D, covariance_type='spherical')\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.head(z)\n",
    "\n",
    "class SoftmaxHead(nn.Module):\n",
    "    def __init__(self, D, C):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(D, C)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.linear(z)\n",
    "\n",
    "class DeepClassifier(nn.Module):\n",
    "    def __init__(self, C, D):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(D)\n",
    "        self.head = SoftmaxHead(D, C)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.head(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd8e3cd",
   "metadata": {},
   "source": [
    "### Train & Eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32f80cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LDA 01] train loss=7.2210 acc=0.2897 | test acc=0.4180\n",
      "[LDA 02] train loss=5.0649 acc=0.4884 | test acc=0.4962\n",
      "[LDA 03] train loss=3.5229 acc=0.5741 | test acc=0.5552\n",
      "[LDA 04] train loss=2.0474 acc=0.6374 | test acc=0.6409\n",
      "[LDA 05] train loss=0.6675 acc=0.6762 | test acc=0.6214\n",
      "[LDA 06] train loss=-0.6390 acc=0.7049 | test acc=0.6410\n",
      "[LDA 07] train loss=-1.6781 acc=0.7375 | test acc=0.7392\n",
      "[LDA 08] train loss=-2.3930 acc=0.7740 | test acc=0.7309\n",
      "[LDA 09] train loss=-2.7373 acc=0.8021 | test acc=0.7463\n",
      "[LDA 10] train loss=-2.9030 acc=0.8215 | test acc=0.7991\n",
      "[LDA 11] train loss=-3.0165 acc=0.8406 | test acc=0.8233\n",
      "[LDA 12] train loss=-3.0935 acc=0.8564 | test acc=0.7695\n",
      "[LDA 13] train loss=-3.1336 acc=0.8659 | test acc=0.8123\n",
      "[LDA 14] train loss=-3.1631 acc=0.8738 | test acc=0.8281\n",
      "[LDA 15] train loss=-3.1936 acc=0.8828 | test acc=0.8374\n",
      "[LDA 16] train loss=-3.2186 acc=0.8894 | test acc=0.8328\n",
      "[LDA 17] train loss=-3.2356 acc=0.8961 | test acc=0.8527\n",
      "[LDA 18] train loss=-3.2483 acc=0.9003 | test acc=0.8479\n",
      "[LDA 19] train loss=-3.2649 acc=0.9048 | test acc=0.8416\n",
      "[LDA 20] train loss=-3.2763 acc=0.9096 | test acc=0.8431\n",
      "[LDA 21] train loss=-3.2888 acc=0.9130 | test acc=0.8526\n",
      "[LDA 22] train loss=-3.2950 acc=0.9151 | test acc=0.8522\n",
      "[LDA 23] train loss=-3.3024 acc=0.9194 | test acc=0.8644\n",
      "[LDA 24] train loss=-3.3209 acc=0.9247 | test acc=0.8384\n",
      "[LDA 25] train loss=-3.3235 acc=0.9269 | test acc=0.8625\n",
      "[LDA 26] train loss=-3.3347 acc=0.9295 | test acc=0.8732\n",
      "[LDA 27] train loss=-3.3448 acc=0.9338 | test acc=0.8685\n",
      "[LDA 28] train loss=-3.3525 acc=0.9377 | test acc=0.8614\n",
      "[LDA 29] train loss=-3.3625 acc=0.9417 | test acc=0.8739\n",
      "[LDA 30] train loss=-3.3662 acc=0.9423 | test acc=0.8741\n",
      "[LDA 31] train loss=-3.3699 acc=0.9436 | test acc=0.8109\n",
      "[LDA 32] train loss=-3.3798 acc=0.9467 | test acc=0.8699\n",
      "[LDA 33] train loss=-3.3883 acc=0.9497 | test acc=0.8599\n",
      "[LDA 34] train loss=-3.3973 acc=0.9531 | test acc=0.8744\n",
      "[LDA 35] train loss=-3.4044 acc=0.9557 | test acc=0.8687\n",
      "[LDA 36] train loss=-3.4018 acc=0.9560 | test acc=0.8776\n",
      "[LDA 37] train loss=-3.4131 acc=0.9581 | test acc=0.8797\n",
      "[LDA 38] train loss=-3.4187 acc=0.9597 | test acc=0.8857\n",
      "[LDA 39] train loss=-3.4275 acc=0.9631 | test acc=0.8764\n",
      "[LDA 40] train loss=-3.4316 acc=0.9650 | test acc=0.8683\n",
      "[LDA 41] train loss=-3.4350 acc=0.9656 | test acc=0.8733\n",
      "[LDA 42] train loss=-3.4403 acc=0.9683 | test acc=0.8766\n",
      "[LDA 43] train loss=-3.4445 acc=0.9689 | test acc=0.8715\n",
      "[LDA 44] train loss=-3.4469 acc=0.9704 | test acc=0.8721\n",
      "[LDA 45] train loss=-3.4517 acc=0.9717 | test acc=0.8815\n",
      "[LDA 46] train loss=-3.4567 acc=0.9741 | test acc=0.8924\n",
      "[LDA 47] train loss=-3.4632 acc=0.9745 | test acc=0.8947\n",
      "[LDA 48] train loss=-3.4624 acc=0.9749 | test acc=0.8904\n",
      "[LDA 49] train loss=-3.4659 acc=0.9758 | test acc=0.8929\n",
      "[LDA 50] train loss=-3.4734 acc=0.9780 | test acc=0.8878\n",
      "[LDA 51] train loss=-3.4772 acc=0.9804 | test acc=0.8925\n",
      "[LDA 52] train loss=-3.4775 acc=0.9792 | test acc=0.8865\n",
      "[LDA 53] train loss=-3.4801 acc=0.9804 | test acc=0.8812\n",
      "[LDA 54] train loss=-3.4849 acc=0.9821 | test acc=0.8872\n",
      "[LDA 55] train loss=-3.4860 acc=0.9820 | test acc=0.8893\n",
      "[LDA 56] train loss=-3.4910 acc=0.9836 | test acc=0.8886\n",
      "[LDA 57] train loss=-3.4908 acc=0.9840 | test acc=0.8889\n",
      "[LDA 58] train loss=-3.4954 acc=0.9856 | test acc=0.8958\n",
      "[LDA 59] train loss=-3.4967 acc=0.9856 | test acc=0.8892\n",
      "[LDA 60] train loss=-3.4973 acc=0.9857 | test acc=0.8969\n",
      "[LDA 61] train loss=-3.5024 acc=0.9868 | test acc=0.8778\n",
      "[LDA 62] train loss=-3.5054 acc=0.9875 | test acc=0.8923\n",
      "[LDA 63] train loss=-3.5077 acc=0.9881 | test acc=0.8885\n",
      "[LDA 64] train loss=-3.5075 acc=0.9879 | test acc=0.8985\n",
      "[LDA 65] train loss=-3.5039 acc=0.9871 | test acc=0.8909\n",
      "[LDA 66] train loss=-3.5097 acc=0.9888 | test acc=0.8982\n",
      "[LDA 67] train loss=-3.5107 acc=0.9878 | test acc=0.8940\n",
      "[LDA 68] train loss=-3.5157 acc=0.9904 | test acc=0.9010\n",
      "[LDA 69] train loss=-3.5194 acc=0.9909 | test acc=0.8983\n",
      "[LDA 70] train loss=-3.5166 acc=0.9905 | test acc=0.8838\n",
      "[LDA 71] train loss=-3.5186 acc=0.9902 | test acc=0.8890\n",
      "[LDA 72] train loss=-3.5235 acc=0.9917 | test acc=0.8967\n",
      "[LDA 73] train loss=-3.5223 acc=0.9914 | test acc=0.8899\n",
      "[LDA 74] train loss=-3.5244 acc=0.9919 | test acc=0.8950\n",
      "[LDA 75] train loss=-3.5251 acc=0.9919 | test acc=0.8991\n",
      "[LDA 76] train loss=-3.5255 acc=0.9918 | test acc=0.9006\n",
      "[LDA 77] train loss=-3.5272 acc=0.9923 | test acc=0.8918\n",
      "[LDA 78] train loss=-3.5257 acc=0.9921 | test acc=0.8982\n",
      "[LDA 79] train loss=-3.5279 acc=0.9926 | test acc=0.8987\n",
      "[LDA 80] train loss=-3.5282 acc=0.9925 | test acc=0.8949\n",
      "[LDA 81] train loss=-3.5325 acc=0.9935 | test acc=0.8992\n",
      "[LDA 82] train loss=-3.5336 acc=0.9935 | test acc=0.8915\n",
      "[LDA 83] train loss=-3.5351 acc=0.9946 | test acc=0.8986\n",
      "[LDA 84] train loss=-3.5374 acc=0.9946 | test acc=0.8983\n",
      "[LDA 85] train loss=-3.5356 acc=0.9938 | test acc=0.8840\n",
      "[LDA 86] train loss=-3.5367 acc=0.9942 | test acc=0.8992\n",
      "[LDA 87] train loss=-3.5366 acc=0.9944 | test acc=0.9014\n",
      "[LDA 88] train loss=-3.5406 acc=0.9953 | test acc=0.8894\n",
      "[LDA 89] train loss=-3.5388 acc=0.9948 | test acc=0.8997\n",
      "[LDA 90] train loss=-3.5435 acc=0.9951 | test acc=0.9008\n",
      "[LDA 91] train loss=-3.5435 acc=0.9954 | test acc=0.9047\n",
      "[LDA 92] train loss=-3.5449 acc=0.9961 | test acc=0.8975\n",
      "[LDA 93] train loss=-3.5451 acc=0.9958 | test acc=0.9034\n",
      "[LDA 94] train loss=-3.5479 acc=0.9962 | test acc=0.9003\n",
      "[LDA 95] train loss=-3.5454 acc=0.9961 | test acc=0.9047\n",
      "[LDA 96] train loss=-3.5453 acc=0.9956 | test acc=0.8987\n",
      "[LDA 97] train loss=-3.5469 acc=0.9961 | test acc=0.9006\n",
      "[LDA 98] train loss=-3.5503 acc=0.9963 | test acc=0.9050\n",
      "[LDA 99] train loss=-3.5450 acc=0.9958 | test acc=0.9049\n",
      "[LDA 100] train loss=-3.5510 acc=0.9968 | test acc=0.9036\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    ok = tot = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        ok += (logits.argmax(1) == y).sum().item()\n",
    "        tot += y.size(0)\n",
    "    return ok / tot\n",
    "\n",
    "lda_model = DeepLDA(C=10, D=9).to(device)\n",
    "lda_opt = torch.optim.Adam(lda_model.parameters())\n",
    "lda_loss_fn = DNLLLoss(lambda_reg=.01)\n",
    "\n",
    "lda_train_acc = []\n",
    "lda_test_acc = []\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    lda_model.train()\n",
    "    loss_sum = acc_sum = n_sum = 0\n",
    "    for x, y in train_ld:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = lda_model(x)\n",
    "        loss = lda_loss_fn(logits, y)\n",
    "        lda_opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        lda_opt.step()\n",
    "        with torch.no_grad():\n",
    "            pred = logits.argmax(1)\n",
    "            acc_sum += (pred == y).sum().item()\n",
    "            n_sum += y.size(0)\n",
    "            loss_sum += loss.item() * y.size(0)\n",
    "    tr_acc = acc_sum / n_sum\n",
    "    te_acc = evaluate(lda_model, test_ld)\n",
    "    lda_train_acc.append(tr_acc)\n",
    "    lda_test_acc.append(te_acc)\n",
    "    print(f\"[LDA {epoch:02d}] train loss={loss_sum/n_sum:.4f} acc={tr_acc:.4f} | test acc={te_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b23943",
   "metadata": {},
   "source": [
    "### Softmax head baseline\n",
    "Train a second model with the same encoder but a standard softmax classifier for a side-by-side comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999aec0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_model = DeepClassifier(C=10, D=9).to(device)\n",
    "softmax_opt = torch.optim.Adam(softmax_model.parameters())\n",
    "softmax_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "softmax_train_acc = []\n",
    "softmax_test_acc = []\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    softmax_model.train()\n",
    "    loss_sum = acc_sum = n_sum = 0\n",
    "    for x, y in train_ld:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = softmax_model(x)\n",
    "        loss = softmax_loss_fn(logits, y)\n",
    "        softmax_opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        softmax_opt.step()\n",
    "        with torch.no_grad():\n",
    "            pred = logits.argmax(1)\n",
    "            acc_sum += (pred == y).sum().item()\n",
    "            n_sum += y.size(0)\n",
    "            loss_sum += loss.item() * y.size(0)\n",
    "    tr_acc = acc_sum / n_sum\n",
    "    te_acc = evaluate(softmax_model, test_ld)\n",
    "    softmax_train_acc.append(tr_acc)\n",
    "    softmax_test_acc.append(te_acc)\n",
    "    print(f\"[Softmax {epoch:02d}] train loss={loss_sum/n_sum:.4f} acc={tr_acc:.4f} | test acc={te_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79716374",
   "metadata": {},
   "source": [
    "### Confidence histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@torch.no_grad()\n",
    "def plot_confidence_hist(model, loader, out_path, title=None):\n",
    "    model.eval()\n",
    "    conf_list, pred_list, label_list = [], [], []\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        conf, pred = probs.max(1)\n",
    "        conf_list.append(conf.cpu())\n",
    "        pred_list.append(pred.cpu())\n",
    "        label_list.append(y.cpu())\n",
    "\n",
    "    conf = torch.cat(conf_list).numpy()\n",
    "    pred = torch.cat(pred_list)\n",
    "    labels = torch.cat(label_list)\n",
    "    acc = (pred == labels).float().mean().item()\n",
    "    avg_conf = conf.mean().item()\n",
    "\n",
    "    bins = np.linspace(0.0, 1.0, 21)\n",
    "    weights = np.ones_like(conf) / conf.size\n",
    "\n",
    "    plt.figure(figsize=(4.5, 4.5))\n",
    "    plt.hist(conf, bins=bins, weights=weights, color='blue', edgecolor='black')\n",
    "    plt.axvline(avg_conf, color='gray', linestyle='--', linewidth=3)\n",
    "    plt.axvline(acc, color='gray', linestyle='--', linewidth=3)\n",
    "    plt.text(avg_conf, 0.95 * plt.gca().get_ylim()[1], 'Avg confidence',\n",
    "             rotation=90, va='top', ha='center')\n",
    "    plt.text(acc, 0.95 * plt.gca().get_ylim()[1], 'Accuracy',\n",
    "             rotation=90, va='top', ha='center')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('% of Samples')\n",
    "    plt.xlim(0, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=600)\n",
    "\n",
    "plot_confidence_hist(lda_model, test_ld, 'plots/cifar10_lda_confidence_hist.png', title='LDA')\n",
    "plot_confidence_hist(softmax_model, test_ld, 'plots/cifar10_softmax_confidence_hist.png', title='Softmax')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reliability diagrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@torch.no_grad()\n",
    "def plot_reliability_diagram(model, loader, out_path, title=None, n_bins=10):\n",
    "    model.eval()\n",
    "    conf_list, pred_list, label_list = [], [], []\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        conf, pred = probs.max(1)\n",
    "        conf_list.append(conf.cpu())\n",
    "        pred_list.append(pred.cpu())\n",
    "        label_list.append(y.cpu())\n",
    "\n",
    "    conf = torch.cat(conf_list).numpy()\n",
    "    pred = torch.cat(pred_list).numpy()\n",
    "    labels = torch.cat(label_list).numpy()\n",
    "    correct = (pred == labels).astype(np.float32)\n",
    "\n",
    "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    bin_ids = np.digitize(conf, bins[1:-1], right=True)\n",
    "\n",
    "    bin_acc = np.zeros(n_bins, dtype=np.float32)\n",
    "    bin_conf = np.zeros(n_bins, dtype=np.float32)\n",
    "    bin_frac = np.zeros(n_bins, dtype=np.float32)\n",
    "\n",
    "    for b in range(n_bins):\n",
    "        mask = bin_ids == b\n",
    "        if mask.any():\n",
    "            bin_acc[b] = correct[mask].mean()\n",
    "            bin_conf[b] = conf[mask].mean()\n",
    "            bin_frac[b] = mask.mean()\n",
    "\n",
    "    ece = np.sum(np.abs(bin_acc - bin_conf) * bin_frac)\n",
    "\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "    bin_width = bins[1] - bins[0]\n",
    "\n",
    "    plt.figure(figsize=(4.5, 4.5))\n",
    "    plt.plot([0, 1], [0, 1], '--', color='gray', linewidth=3)\n",
    "    plt.bar(bin_centers, bin_acc, width=bin_width, color='blue', edgecolor='black', label='Outputs')\n",
    "    gap = bin_conf - bin_acc\n",
    "    plt.bar(bin_centers, gap, bottom=bin_acc, width=bin_width, color='salmon', edgecolor='red',\n",
    "            alpha=0.4, hatch='//', label='Gap')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.text(0.95, 0.05, f'ECE={ece*100:.2f}', ha='right', va='bottom',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightsteelblue', alpha=0.8))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=600)\n",
    "\n",
    "plot_reliability_diagram(lda_model, test_ld, 'plots/cifar10_lda_reliability_diagram.png', title='LDA')\n",
    "plot_reliability_diagram(softmax_model, test_ld, 'plots/cifar10_softmax_reliability_diagram.png', title='Softmax')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be52016b",
   "metadata": {},
   "source": [
    "### PCA over Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d812f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_embeddings(model, loader, batches=10):\n",
    "    model.eval()\n",
    "    embeds, labels = [], []\n",
    "    for i, (x, y) in enumerate(loader):   # use train_ld if you prefer\n",
    "        x = x.to(device)\n",
    "        z = model.encoder(x).cpu()\n",
    "        embeds.append(z)\n",
    "        labels.append(y)\n",
    "        if i >= batches - 1:   # 10 batches ≈10k points; raise/lower to taste\n",
    "            break\n",
    "\n",
    "    z = torch.cat(embeds)\n",
    "    y = torch.cat(labels)\n",
    "\n",
    "    # 2D projection (PCA)\n",
    "    z0 = z - z.mean(0, keepdim=True)\n",
    "    _, _, V = torch.pca_lowrank(z0, q=2)\n",
    "    z2 = z0 @ V[:, :2]\n",
    "    return z2, y\n",
    "\n",
    "lda_z2, lda_y = collect_embeddings(lda_model, train_ld)\n",
    "softmax_z2, softmax_y = collect_embeddings(softmax_model, train_ld)\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 14, \"axes.labelsize\": 16, \"legend.fontsize\": 13,\n",
    "    \"xtick.labelsize\": 13, \"ytick.labelsize\": 13\n",
    "})\n",
    "\n",
    "palette = plt.cm.tab10.colors\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "plots = [\n",
    "    (axes[0], lda_z2, lda_y, \"Deep LDA embeddings\"),\n",
    "    (axes[1], softmax_z2, softmax_y, \"Softmax head embeddings\"),\n",
    "]\n",
    "\n",
    "for ax, z2, y, title in plots:\n",
    "    for c in range(10):\n",
    "        idx = y == c\n",
    "        ax.scatter(z2[idx, 0], z2[idx, 1], s=8, alpha=0.6, color=palette[c], label=train_ds.classes[c])\n",
    "    ax.set_xlabel(\"PC1\")\n",
    "    ax.set_ylabel(\"PC2\")\n",
    "    ax.set_title(title)\n",
    "\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, bbox_to_anchor=(0.5, -0.02), loc=\"lower center\", ncol=5)\n",
    "plt.tight_layout(rect=(0, 0.07, 1, 1))\n",
    "plt.savefig('plots/cifar10_lda_vs_softmax_embeddings.png', dpi=600)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0802afa3",
   "metadata": {},
   "source": [
    "# CIFAR-100 Classification with an LDA Head\n",
    "This notebook trains a lightweight convolutional encoder with a linear discriminant analysis (LDA) head on CIFAR-100, then visualises the learned embedding space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c1fc5d",
   "metadata": {},
   "source": [
    "### Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf11f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from dnll import LDAHead, DNLLLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab011bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device =', device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7881674",
   "metadata": {},
   "source": [
    "### Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8e436e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = (0.5071, 0.4867, 0.4408)\n",
    "std = (0.2675, 0.2565, 0.2761)\n",
    "pin_memory = torch.cuda.is_available()\n",
    "\n",
    "train_tfm = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "test_tfm = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "train_ds = datasets.CIFAR100(root='./data', train=True, transform=train_tfm, download=True)\n",
    "test_ds  = datasets.CIFAR100(root='./data', train=False, transform=test_tfm, download=True)\n",
    "train_ld = DataLoader(train_ds, batch_size=256, shuffle=True, num_workers=4, pin_memory=pin_memory)\n",
    "test_ld  = DataLoader(test_ds,  batch_size=1024, shuffle=False, num_workers=4, pin_memory=pin_memory)\n",
    "len(train_ds), len(test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0efcdea",
   "metadata": {},
   "source": [
    "### Model: encoder + LDA head (on-the-fly stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daebb93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "        )\n",
    "        self.proj = nn.Linear(256, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.proj(x)\n",
    "\n",
    "class DeepLDA(nn.Module):\n",
    "    def __init__(self, C, D):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(D)\n",
    "        self.head = LDAHead(C, D)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.head(z)\n",
    "\n",
    "class SoftmaxHead(nn.Module):\n",
    "    def __init__(self, D, C):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(D, C)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.linear(z)\n",
    "\n",
    "class DeepClassifier(nn.Module):\n",
    "    def __init__(self, C, D):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(D)\n",
    "        self.head = SoftmaxHead(D, C)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.head(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd8e3cd",
   "metadata": {},
   "source": [
    "### Train & Eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f80cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    ok = tot = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        ok += (logits.argmax(1) == y).sum().item()\n",
    "        tot += y.size(0)\n",
    "    return ok / tot\n",
    "\n",
    "model = DeepLDA(C=100, D=99).to(device)\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "#loss_fn = nn.CrossEntropyLoss()\n",
    "#loss_fn = nn.NLLLoss()\n",
    "#loss_fn = LDALoss()\n",
    "#loss_fn = nn.PoissonNLLLoss()\n",
    "#loss_fn = LogisticLoss()\n",
    "loss_fn = DNLLLoss(lambda_reg=.01)\n",
    "\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    model.train()\n",
    "    loss_sum = acc_sum = n_sum = 0\n",
    "    for x, y in train_ld:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        #y_oh = 100 * F.one_hot(y, num_classes=100).to(logits.dtype)  # shape [batch,100]\n",
    "        loss = loss_fn(logits, y)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        with torch.no_grad():\n",
    "            pred = logits.argmax(1)\n",
    "            acc_sum += (pred == y).sum().item()\n",
    "            n_sum += y.size(0)\n",
    "            loss_sum += loss.item() * y.size(0)\n",
    "    tr_acc = acc_sum / n_sum\n",
    "    te_acc = evaluate(model, test_ld)\n",
    "    train_acc.append(tr_acc)\n",
    "    test_acc.append(te_acc)\n",
    "    print(f\"[{epoch:02d}] train loss={loss_sum/n_sum:.4f} acc={tr_acc:.4f} | test acc={te_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax head baseline\n",
    "Train a second model with the same encoder but a standard softmax classifier for a side-by-side comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_model = DeepClassifier(C=100, D=99).to(device)\n",
    "softmax_opt = torch.optim.Adam(softmax_model.parameters())\n",
    "softmax_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "softmax_train_acc = []\n",
    "softmax_test_acc = []\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    softmax_model.train()\n",
    "    loss_sum = acc_sum = n_sum = 0\n",
    "    for x, y in train_ld:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = softmax_model(x)\n",
    "        loss = softmax_loss_fn(logits, y)\n",
    "        softmax_opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        softmax_opt.step()\n",
    "        with torch.no_grad():\n",
    "            pred = logits.argmax(1)\n",
    "            acc_sum += (pred == y).sum().item()\n",
    "            n_sum += y.size(0)\n",
    "            loss_sum += loss.item() * y.size(0)\n",
    "    tr_acc = acc_sum / n_sum\n",
    "    te_acc = evaluate(softmax_model, test_ld)\n",
    "    softmax_train_acc.append(tr_acc)\n",
    "    softmax_test_acc.append(te_acc)\n",
    "    print(f\"[Softmax {epoch:02d}] train loss={loss_sum/n_sum:.4f} acc={tr_acc:.4f} | test acc={te_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f4c333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "conf_list = []\n",
    "pred_list = []\n",
    "label_list = []\n",
    "with torch.no_grad():\n",
    "    for x, y in test_ld:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        conf, pred = probs.max(1)\n",
    "        conf_list.append(conf.cpu())\n",
    "        pred_list.append(pred.cpu())\n",
    "        label_list.append(y.cpu())\n",
    "\n",
    "conf = torch.cat(conf_list).numpy()\n",
    "pred = torch.cat(pred_list)\n",
    "labels = torch.cat(label_list)\n",
    "acc = (pred == labels).float().mean().item()\n",
    "avg_conf = conf.mean().item()\n",
    "\n",
    "bins = np.linspace(0.0, 1.0, 21)\n",
    "weights = np.ones_like(conf) / conf.size\n",
    "\n",
    "plt.figure(figsize=(4.5, 4.5))\n",
    "plt.hist(conf, bins=bins, weights=weights, color='blue', edgecolor='black')\n",
    "plt.axvline(avg_conf, color='gray', linestyle='--', linewidth=3)\n",
    "plt.axvline(acc, color='gray', linestyle='--', linewidth=3)\n",
    "plt.text(avg_conf, 0.95 * plt.gca().get_ylim()[1], 'Avg confidence',\n",
    "         rotation=90, va='top', ha='center')\n",
    "plt.text(acc, 0.95 * plt.gca().get_ylim()[1], 'Accuracy',\n",
    "         rotation=90, va='top', ha='center')\n",
    "plt.xlabel('Confidence')\n",
    "plt.ylabel('% of Samples')\n",
    "plt.xlim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/cifar100_confidence_hist.png', dpi=600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013d4195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "conf_list = []\n",
    "pred_list = []\n",
    "label_list = []\n",
    "with torch.no_grad():\n",
    "    for x, y in test_ld:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        conf, pred = probs.max(1)\n",
    "        conf_list.append(conf.cpu())\n",
    "        pred_list.append(pred.cpu())\n",
    "        label_list.append(y.cpu())\n",
    "\n",
    "conf = torch.cat(conf_list).numpy()\n",
    "pred = torch.cat(pred_list).numpy()\n",
    "labels = torch.cat(label_list).numpy()\n",
    "correct = (pred == labels).astype(np.float32)\n",
    "\n",
    "n_bins = 10\n",
    "bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "bin_ids = np.digitize(conf, bins[1:-1], right=True)\n",
    "\n",
    "bin_acc = np.zeros(n_bins, dtype=np.float32)\n",
    "bin_conf = np.zeros(n_bins, dtype=np.float32)\n",
    "bin_frac = np.zeros(n_bins, dtype=np.float32)\n",
    "\n",
    "for b in range(n_bins):\n",
    "    mask = bin_ids == b\n",
    "    if mask.any():\n",
    "        bin_acc[b] = correct[mask].mean()\n",
    "        bin_conf[b] = conf[mask].mean()\n",
    "        bin_frac[b] = mask.mean()\n",
    "\n",
    "ece = np.sum(np.abs(bin_acc - bin_conf) * bin_frac)\n",
    "\n",
    "bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "bin_width = bins[1] - bins[0]\n",
    "\n",
    "plt.figure(figsize=(4.5, 4.5))\n",
    "plt.plot([0, 1], [0, 1], '--', color='gray', linewidth=3)\n",
    "plt.bar(bin_centers, bin_acc, width=bin_width, color='blue', edgecolor='black', label='Outputs')\n",
    "gap = bin_conf - bin_acc\n",
    "plt.bar(bin_centers, gap, bottom=bin_acc, width=bin_width, color='salmon', edgecolor='red',\n",
    "        alpha=0.4, hatch='//', label='Gap')\n",
    "plt.xlabel('Confidence')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(loc='upper left')\n",
    "plt.text(0.95, 0.05, f'ECE={ece*100:.2f}', ha='right', va='bottom',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightsteelblue', alpha=0.8))\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/cifar100_reliability_diagram.png', dpi=600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff1de92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, len(train_acc) + 1)\n",
    "plt.figure(figsize=(6, 4))\n",
    "#plt.plot(epochs, train_acc, label='Train accuracy')\n",
    "plt.plot(epochs, test_acc, label='Test accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('CIFAR-100 accuracy during training')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff97720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "save_path = 'cifar100_model.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': opt.state_dict(),\n",
    "}, save_path)\n",
    "print(f'Saved model checkpoint to {save_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA over Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_embeddings(model, loader, batches=10):\n",
    "    model.eval()\n",
    "    embeds, labels = [], []\n",
    "    for i, (x, y) in enumerate(loader):   # use train_ld if you prefer\n",
    "        x = x.to(device)\n",
    "        z = model.encoder(x).cpu()\n",
    "        embeds.append(z)\n",
    "        labels.append(y)\n",
    "        if i >= batches - 1:   # 10 batches ~10k points; raise/lower to taste\n",
    "            break\n",
    "\n",
    "    z = torch.cat(embeds)\n",
    "    y = torch.cat(labels)\n",
    "\n",
    "    # 2D projection (PCA)\n",
    "    z0 = z - z.mean(0, keepdim=True)\n",
    "    _, _, V = torch.pca_lowrank(z0, q=2)\n",
    "    z2 = z0 @ V[:, :2]\n",
    "    return z2, y\n",
    "\n",
    "lda_model = model\n",
    "if 'softmax_model' not in globals():\n",
    "    raise RuntimeError('softmax_model not found. Train or load a Softmax baseline before running this cell.')\n",
    "lda_z2, lda_y = collect_embeddings(lda_model, train_ld)\n",
    "softmax_z2, softmax_y = collect_embeddings(softmax_model, train_ld)\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 14, \"axes.labelsize\": 16, \"legend.fontsize\": 13,\n",
    "    \"xtick.labelsize\": 13, \"ytick.labelsize\": 13\n",
    "})\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "plots = [\n",
    "    (axes[0], lda_z2, lda_y, \"Deep LDA embeddings\"),\n",
    "    (axes[1], softmax_z2, softmax_y, \"Softmax head embeddings\"),\n",
    "]\n",
    "\n",
    "for ax, z2, y, title in plots:\n",
    "    sc = ax.scatter(z2[:, 0], z2[:, 1], c=y, cmap=\"nipy_spectral\",\n",
    "                    s=8, alpha=0.6, vmin=0, vmax=99, rasterized=True)\n",
    "    ax.set_xlabel(\"PC1\")\n",
    "    ax.set_ylabel(\"PC2\")\n",
    "    ax.set_title(title)\n",
    "\n",
    "cbar = fig.colorbar(sc, ax=axes, fraction=0.046, pad=0.04)\n",
    "cbar.set_label(\"Class index\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/cifar100_lda_vs_softmax_embeddings.png', dpi=600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea49e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a few examples from the two closest classes\n",
    "import numpy as np\n",
    "\n",
    "# Find closest pair using class means from the previous cell\n",
    "with torch.no_grad():\n",
    "    dist_mat = torch.cdist(class_means, class_means)\n",
    "    dist_mat.fill_diagonal_(float('inf'))\n",
    "    row_min, row_argmin = dist_mat.min(dim=1)\n",
    "    cls_a = row_min.argmin().item()\n",
    "    cls_b = row_argmin[cls_a].item()\n",
    "\n",
    "cls_names = train_ds.classes\n",
    "print(f\"Closest classes: {cls_a} ({cls_names[cls_a]}) and {cls_b} ({cls_names[cls_b]}), distance={dist_mat[cls_a, cls_b]:.4f}\")\n",
    "\n",
    "# Collect a few raw images (no augmentation) for each class from the training set\n",
    "n_per_class = 4\n",
    "indices_a = [i for i, t in enumerate(train_ds.targets) if t == cls_a][:n_per_class]\n",
    "indices_b = [i for i, t in enumerate(train_ds.targets) if t == cls_b][:n_per_class]\n",
    "\n",
    "fig, axes = plt.subplots(2, n_per_class, figsize=(2 * n_per_class, 4))\n",
    "for j, idx in enumerate(indices_a):\n",
    "    axes[0, j].imshow(train_ds.data[idx])\n",
    "    axes[0, j].axis('off')\n",
    "    axes[0, j].set_title(f\"{cls_names[cls_a]}\")\n",
    "for j, idx in enumerate(indices_b):\n",
    "    axes[1, j].imshow(train_ds.data[idx])\n",
    "    axes[1, j].axis('off')\n",
    "    axes[1, j].set_title(f\"{cls_names[cls_b]}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc8716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show examples from the 2nd closest pair of classes\n",
    "# Assumes class_means computed above\n",
    "with torch.no_grad():\n",
    "    dist_mat = torch.cdist(class_means, class_means)\n",
    "    dist_mat.fill_diagonal_(float('inf'))\n",
    "    i_idx, j_idx = torch.triu_indices(dist_mat.size(0), dist_mat.size(1), offset=1)\n",
    "    pair_dists = dist_mat[i_idx, j_idx]\n",
    "    if pair_dists.numel() < 2:\n",
    "        raise RuntimeError('Need at least two class pairs to find the 2nd closest pair.')\n",
    "    sorted_vals, sorted_idx = torch.sort(pair_dists)\n",
    "    second_idx = sorted_idx[1].item()\n",
    "    cls_a = i_idx[second_idx].item()\n",
    "    cls_b = j_idx[second_idx].item()\n",
    "    pair_dist = sorted_vals[1].item()\n",
    "\n",
    "cls_names = train_ds.classes\n",
    "print(f\"2nd closest classes: {cls_a} ({cls_names[cls_a]}) and {cls_b} ({cls_names[cls_b]}), distance={pair_dist:.4f}\")\n",
    "\n",
    "n_per_class = 4\n",
    "indices_a = [i for i, t in enumerate(train_ds.targets) if t == cls_a][:n_per_class]\n",
    "indices_b = [i for i, t in enumerate(train_ds.targets) if t == cls_b][:n_per_class]\n",
    "\n",
    "fig, axes = plt.subplots(2, n_per_class, figsize=(2 * n_per_class, 4))\n",
    "for j, idx in enumerate(indices_a):\n",
    "    axes[0, j].imshow(train_ds.data[idx])\n",
    "    axes[0, j].axis('off')\n",
    "    axes[0, j].set_title(f\"{cls_names[cls_a]}\")\n",
    "for j, idx in enumerate(indices_b):\n",
    "    axes[1, j].imshow(train_ds.data[idx])\n",
    "    axes[1, j].axis('off')\n",
    "    axes[1, j].set_title(f\"{cls_names[cls_b]}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09d6f7f",
   "metadata": {},
   "source": [
    "# Fashion-MNIST Classification with an LDA Head\n",
    "This notebook trains a small convolutional encoder with a linear discriminant analysis (LDA) head on Fashion-MNIST, then visualises the learned embedding space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f89626",
   "metadata": {},
   "source": [
    "### Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80764dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from dnll import LDAHead, DNLLLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd667682",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device =', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc74c579",
   "metadata": {},
   "source": [
    "### Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbdd16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm = transforms.ToTensor()\n",
    "train_ds = datasets.FashionMNIST(root='./data', train=True, transform=tfm, download=True)\n",
    "test_ds  = datasets.FashionMNIST(root='./data', train=False, transform=tfm, download=True)\n",
    "train_ld = DataLoader(train_ds, batch_size=256, shuffle=True, num_workers=2, pin_memory=True)\n",
    "test_ld  = DataLoader(test_ds,  batch_size=1024, shuffle=False, num_workers=2, pin_memory=True)\n",
    "len(train_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaab3fc",
   "metadata": {},
   "source": [
    "### Model: encoder + LDA head (on-the-fly stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a641b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "        )\n",
    "        self.proj = nn.Linear(256, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.proj(x)\n",
    "\n",
    "class DeepLDA(nn.Module):\n",
    "    def __init__(self, C, D):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(D)\n",
    "        self.head = LDAHead(C, D)\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.head(z)\n",
    "\n",
    "class SoftmaxHead(nn.Module):\n",
    "    def __init__(self, D, C):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(D, C)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.linear(z)\n",
    "\n",
    "class DeepClassifier(nn.Module):\n",
    "    def __init__(self, C, D):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(D)\n",
    "        self.head = SoftmaxHead(D, C)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.head(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bc29e0",
   "metadata": {},
   "source": [
    "### Train & Eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8039b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    ok = tot = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)  # EMA stats\n",
    "        ok += (logits.argmax(1) == y).sum().item()\n",
    "        tot += y.size(0)\n",
    "    return ok / tot\n",
    "\n",
    "model = DeepLDA(C=10, D=9).to(device)\n",
    "opt = torch.optim.Adam(model.encoder.parameters())\n",
    "#loss_fn = nn.CrossEntropyLoss()\n",
    "#loss_fn = nn.NLLLoss()\n",
    "#loss_fn = LDALoss()\n",
    "loss_fn = DNLLLoss(lambda_reg=.01)\n",
    "#loss_fn = LogisticLoss()\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    model.train()\n",
    "    loss_sum = acc_sum = n_sum = 0\n",
    "    for x, y in train_ld:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)        # uses batch stats + EMA update\n",
    "        loss = loss_fn(logits, y)\n",
    "        opt.zero_grad(set_to_none=True); loss.backward(); opt.step()\n",
    "        with torch.no_grad():\n",
    "            pred = logits.argmax(1)\n",
    "            acc_sum += (pred == y).sum().item()\n",
    "            n_sum += y.size(0)\n",
    "            loss_sum += loss.item() * y.size(0)\n",
    "    tr_acc = acc_sum / n_sum\n",
    "    te_acc = evaluate(model, test_ld)\n",
    "    print(f\"[{epoch:02d}] train loss={loss_sum/n_sum:.4f} acc={tr_acc:.4f} | test acc={te_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff47189b",
   "metadata": {},
   "source": [
    "### Softmax head baseline\n",
    "Train a second model with the same encoder but a standard softmax classifier for a side-by-side comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493d1935",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_model = DeepClassifier(C=10, D=9).to(device)\n",
    "softmax_opt = torch.optim.Adam(softmax_model.parameters())\n",
    "softmax_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "softmax_train_acc = []\n",
    "softmax_test_acc = []\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    softmax_model.train()\n",
    "    loss_sum = acc_sum = n_sum = 0\n",
    "    for x, y in train_ld:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = softmax_model(x)\n",
    "        loss = softmax_loss_fn(logits, y)\n",
    "        softmax_opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        softmax_opt.step()\n",
    "        with torch.no_grad():\n",
    "            pred = logits.argmax(1)\n",
    "            acc_sum += (pred == y).sum().item()\n",
    "            n_sum += y.size(0)\n",
    "            loss_sum += loss.item() * y.size(0)\n",
    "    tr_acc = acc_sum / n_sum\n",
    "    te_acc = evaluate(softmax_model, test_ld)\n",
    "    softmax_train_acc.append(tr_acc)\n",
    "    softmax_test_acc.append(te_acc)\n",
    "    print(f\"[Softmax {epoch:02d}] train loss={loss_sum/n_sum:.4f} acc={tr_acc:.4f} | test acc={te_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e642bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@torch.no_grad()\n",
    "def plot_confidence_hist(model, loader, out_path, title=None):\n",
    "    model.eval()\n",
    "    conf_list, pred_list, label_list = [], [], []\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        conf, pred = probs.max(1)\n",
    "        conf_list.append(conf.cpu())\n",
    "        pred_list.append(pred.cpu())\n",
    "        label_list.append(y.cpu())\n",
    "\n",
    "    conf = torch.cat(conf_list).numpy()\n",
    "    pred = torch.cat(pred_list)\n",
    "    labels = torch.cat(label_list)\n",
    "    acc = (pred == labels).float().mean().item()\n",
    "    avg_conf = conf.mean().item()\n",
    "\n",
    "    bins = np.linspace(0.0, 1.0, 21)\n",
    "    weights = np.ones_like(conf) / conf.size\n",
    "\n",
    "    plt.figure(figsize=(4.5, 4.5))\n",
    "    plt.hist(conf, bins=bins, weights=weights, color='blue', edgecolor='black')\n",
    "    plt.axvline(avg_conf, color='gray', linestyle='--', linewidth=3)\n",
    "    plt.axvline(acc, color='gray', linestyle='--', linewidth=3)\n",
    "    plt.text(avg_conf, 0.95 * plt.gca().get_ylim()[1], 'Avg confidence',\n",
    "             rotation=90, va='top', ha='center')\n",
    "    plt.text(acc, 0.95 * plt.gca().get_ylim()[1], 'Accuracy',\n",
    "             rotation=90, va='top', ha='center')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('% of Samples')\n",
    "    plt.xlim(0, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=600)\n",
    "\n",
    "plot_confidence_hist(model, test_ld, 'plots/fashion_mnist_lda_confidence_hist.png', title='LDA')\n",
    "plot_confidence_hist(softmax_model, test_ld, 'plots/fashion_mnist_softmax_confidence_hist.png', title='Softmax')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb936cc0",
   "metadata": {},
   "source": [
    "### Reliability diagram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9235af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@torch.no_grad()\n",
    "def plot_reliability_diagram(model, loader, out_path, title=None, n_bins=10):\n",
    "    model.eval()\n",
    "    conf_list, pred_list, label_list = [], [], []\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        conf, pred = probs.max(1)\n",
    "        conf_list.append(conf.cpu())\n",
    "        pred_list.append(pred.cpu())\n",
    "        label_list.append(y.cpu())\n",
    "\n",
    "    conf = torch.cat(conf_list).numpy()\n",
    "    pred = torch.cat(pred_list).numpy()\n",
    "    labels = torch.cat(label_list).numpy()\n",
    "    correct = (pred == labels).astype(np.float32)\n",
    "\n",
    "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    bin_ids = np.digitize(conf, bins[1:-1], right=True)\n",
    "\n",
    "    bin_acc = np.zeros(n_bins, dtype=np.float32)\n",
    "    bin_conf = np.zeros(n_bins, dtype=np.float32)\n",
    "    bin_frac = np.zeros(n_bins, dtype=np.float32)\n",
    "\n",
    "    for b in range(n_bins):\n",
    "        mask = bin_ids == b\n",
    "        if mask.any():\n",
    "            bin_acc[b] = correct[mask].mean()\n",
    "            bin_conf[b] = conf[mask].mean()\n",
    "            bin_frac[b] = mask.mean()\n",
    "\n",
    "    ece = np.sum(np.abs(bin_acc - bin_conf) * bin_frac)\n",
    "\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "    bin_width = bins[1] - bins[0]\n",
    "\n",
    "    plt.figure(figsize=(4.5, 4.5))\n",
    "    plt.plot([0, 1], [0, 1], '--', color='gray', linewidth=3)\n",
    "    plt.bar(bin_centers, bin_acc, width=bin_width, color='blue', edgecolor='black', label='Outputs')\n",
    "    gap = bin_conf - bin_acc\n",
    "    plt.bar(bin_centers, gap, bottom=bin_acc, width=bin_width, color='salmon', edgecolor='red',\n",
    "            alpha=0.4, hatch='//', label='Gap')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.text(0.95, 0.05, f'ECE={ece*100:.2f}', ha='right', va='bottom',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightsteelblue', alpha=0.8))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=600)\n",
    "\n",
    "plot_reliability_diagram(model, test_ld, 'plots/fashion_mnist_lda_reliability_diagram.png', title='LDA')\n",
    "plot_reliability_diagram(softmax_model, test_ld, 'plots/fashion_mnist_softmax_reliability_diagram.png', title='Softmax')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f289ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_embeddings(model, loader, batches=10):\n",
    "    model.eval()\n",
    "    embeds, labels = [], []\n",
    "    for i, (x, y) in enumerate(loader):   # use train_ld if you prefer\n",
    "        x = x.to(device)\n",
    "        z = model.encoder(x).cpu()\n",
    "        embeds.append(z)\n",
    "        labels.append(y)\n",
    "        if i >= batches - 1:   # 10 batches ~10k points; raise/lower to taste\n",
    "            break\n",
    "\n",
    "    z = torch.cat(embeds)\n",
    "    y = torch.cat(labels)\n",
    "\n",
    "    # 2D projection (PCA)\n",
    "    z0 = z - z.mean(0, keepdim=True)\n",
    "    _, _, V = torch.pca_lowrank(z0, q=2)\n",
    "    z2 = z0 @ V[:, :2]\n",
    "    return z2, y\n",
    "\n",
    "lda_model = model\n",
    "if 'softmax_model' not in globals():\n",
    "    raise RuntimeError('softmax_model not found. Train or load a Softmax baseline before running this cell.')\n",
    "lda_z2, lda_y = collect_embeddings(lda_model, train_ld)\n",
    "softmax_z2, softmax_y = collect_embeddings(softmax_model, train_ld)\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 14, \"axes.labelsize\": 16, \"legend.fontsize\": 13,\n",
    "    \"xtick.labelsize\": 13, \"ytick.labelsize\": 13\n",
    "})\n",
    "\n",
    "palette = plt.cm.tab10.colors\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "plots = [\n",
    "    (axes[1], lda_z2, lda_y, \"LDA\"),\n",
    "    (axes[0], softmax_z2, softmax_y, \"Softmax\"),\n",
    "]\n",
    "\n",
    "for ax, z2, y, title in plots:\n",
    "    for c in range(10):\n",
    "        idx = y == c\n",
    "        ax.scatter(z2[idx, 0], z2[idx, 1], s=8, alpha=0.6, color=palette[c], label=train_ds.classes[c])\n",
    "    ax.set_xlabel(\"PC1\")\n",
    "    ax.set_ylabel(\"PC2\")\n",
    "    ax.set_title(title)\n",
    "\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, bbox_to_anchor=(0.5, -0.02), loc=\"lower center\", ncol=5)\n",
    "plt.tight_layout(rect=(0, 0.07, 1, 1))\n",
    "plt.savefig('plots/fashion_mnist_lda_vs_softmax_embeddings.png', dpi=600)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
