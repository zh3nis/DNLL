{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0802afa3",
   "metadata": {},
   "source": [
    "# CIFAR-10 Classification with an LDA Head\n",
    "This notebook trains a lightweight convolutional encoder with an LDA head on CIFAR-10, alongside a softmax baseline, then visualises the learned embedding spaces side by side."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c1fc5d",
   "metadata": {},
   "source": [
    "### Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf11f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from dnll import LDAHead, DNLLLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab011bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device =', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7881674",
   "metadata": {},
   "source": [
    "### Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8e436e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = (0.4914, 0.4822, 0.4465)\n",
    "std = (0.2470, 0.2435, 0.2616)\n",
    "pin_memory = torch.cuda.is_available()\n",
    "\n",
    "train_tfm = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "test_tfm = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "train_ds = datasets.CIFAR10(root='./data', train=True, transform=train_tfm, download=True)\n",
    "test_ds  = datasets.CIFAR10(root='./data', train=False, transform=test_tfm, download=True)\n",
    "train_ld = DataLoader(train_ds, batch_size=256, shuffle=True, num_workers=4, pin_memory=pin_memory)\n",
    "test_ld  = DataLoader(test_ds,  batch_size=1024, shuffle=False, num_workers=4, pin_memory=pin_memory)\n",
    "len(train_ds), len(test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0efcdea",
   "metadata": {},
   "source": [
    "### Model: encoder + heads (LDA + softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daebb93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "        )\n",
    "        self.proj = nn.Linear(256, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.proj(x)\n",
    "\n",
    "class DeepLDA(nn.Module):\n",
    "    def __init__(self, C, D):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(D)\n",
    "        self.head = LDAHead(C, D, covariance_type='spherical')\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.head(z)\n",
    "\n",
    "class SoftmaxHead(nn.Module):\n",
    "    def __init__(self, D, C):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(D, C)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.linear(z)\n",
    "\n",
    "class DeepClassifier(nn.Module):\n",
    "    def __init__(self, C, D):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(D)\n",
    "        self.head = SoftmaxHead(D, C)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.head(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd8e3cd",
   "metadata": {},
   "source": [
    "### Train & Eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f80cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    ok = tot = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        ok += (logits.argmax(1) == y).sum().item()\n",
    "        tot += y.size(0)\n",
    "    return ok / tot\n",
    "\n",
    "lda_model = DeepLDA(C=10, D=9).to(device)\n",
    "lda_opt = torch.optim.Adam(lda_model.parameters())\n",
    "lda_loss_fn = DNLLLoss(lambda_reg=.01)\n",
    "\n",
    "lda_train_acc = []\n",
    "lda_test_acc = []\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    lda_model.train()\n",
    "    loss_sum = acc_sum = n_sum = 0\n",
    "    for x, y in train_ld:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = lda_model(x)\n",
    "        loss = lda_loss_fn(logits, y)\n",
    "        lda_opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        lda_opt.step()\n",
    "        with torch.no_grad():\n",
    "            pred = logits.argmax(1)\n",
    "            acc_sum += (pred == y).sum().item()\n",
    "            n_sum += y.size(0)\n",
    "            loss_sum += loss.item() * y.size(0)\n",
    "    tr_acc = acc_sum / n_sum\n",
    "    te_acc = evaluate(lda_model, test_ld)\n",
    "    lda_train_acc.append(tr_acc)\n",
    "    lda_test_acc.append(te_acc)\n",
    "    print(f\"[LDA {epoch:02d}] train loss={loss_sum/n_sum:.4f} acc={tr_acc:.4f} | test acc={te_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b23943",
   "metadata": {},
   "source": [
    "### Softmax head baseline\n",
    "Train a second model with the same encoder but a standard softmax classifier for a side-by-side comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999aec0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_model = DeepClassifier(C=10, D=9).to(device)\n",
    "softmax_opt = torch.optim.Adam(softmax_model.parameters())\n",
    "softmax_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "softmax_train_acc = []\n",
    "softmax_test_acc = []\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    softmax_model.train()\n",
    "    loss_sum = acc_sum = n_sum = 0\n",
    "    for x, y in train_ld:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = softmax_model(x)\n",
    "        loss = softmax_loss_fn(logits, y)\n",
    "        softmax_opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        softmax_opt.step()\n",
    "        with torch.no_grad():\n",
    "            pred = logits.argmax(1)\n",
    "            acc_sum += (pred == y).sum().item()\n",
    "            n_sum += y.size(0)\n",
    "            loss_sum += loss.item() * y.size(0)\n",
    "    tr_acc = acc_sum / n_sum\n",
    "    te_acc = evaluate(softmax_model, test_ld)\n",
    "    softmax_train_acc.append(tr_acc)\n",
    "    softmax_test_acc.append(te_acc)\n",
    "    print(f\"[Softmax {epoch:02d}] train loss={loss_sum/n_sum:.4f} acc={tr_acc:.4f} | test acc={te_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79716374",
   "metadata": {},
   "source": [
    "### Confidence histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@torch.no_grad()\n",
    "def plot_confidence_hist(model, loader, out_path, title=None):\n",
    "    model.eval()\n",
    "    conf_list, pred_list, label_list = [], [], []\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        conf, pred = probs.max(1)\n",
    "        conf_list.append(conf.cpu())\n",
    "        pred_list.append(pred.cpu())\n",
    "        label_list.append(y.cpu())\n",
    "\n",
    "    conf = torch.cat(conf_list).numpy()\n",
    "    pred = torch.cat(pred_list)\n",
    "    labels = torch.cat(label_list)\n",
    "    acc = (pred == labels).float().mean().item()\n",
    "    avg_conf = conf.mean().item()\n",
    "\n",
    "    bins = np.linspace(0.0, 1.0, 21)\n",
    "    weights = np.ones_like(conf) / conf.size\n",
    "\n",
    "    plt.figure(figsize=(4.5, 4.5))\n",
    "    plt.hist(conf, bins=bins, weights=weights, color='blue', edgecolor='black')\n",
    "    plt.axvline(avg_conf, color='gray', linestyle='--', linewidth=3)\n",
    "    plt.axvline(acc, color='gray', linestyle='--', linewidth=3)\n",
    "    plt.text(avg_conf, 0.95 * plt.gca().get_ylim()[1], 'Avg confidence',\n",
    "             rotation=90, va='top', ha='center')\n",
    "    plt.text(acc, 0.95 * plt.gca().get_ylim()[1], 'Accuracy',\n",
    "             rotation=90, va='top', ha='center')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('% of Samples')\n",
    "    plt.xlim(0, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=600)\n",
    "\n",
    "plot_confidence_hist(lda_model, test_ld, 'plots/cifar10_lda_confidence_hist.png', title='LDA')\n",
    "plot_confidence_hist(softmax_model, test_ld, 'plots/cifar10_softmax_confidence_hist.png', title='Softmax')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reliability diagrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@torch.no_grad()\n",
    "def plot_reliability_diagram(model, loader, out_path, title=None, n_bins=10):\n",
    "    model.eval()\n",
    "    conf_list, pred_list, label_list = [], [], []\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        conf, pred = probs.max(1)\n",
    "        conf_list.append(conf.cpu())\n",
    "        pred_list.append(pred.cpu())\n",
    "        label_list.append(y.cpu())\n",
    "\n",
    "    conf = torch.cat(conf_list).numpy()\n",
    "    pred = torch.cat(pred_list).numpy()\n",
    "    labels = torch.cat(label_list).numpy()\n",
    "    correct = (pred == labels).astype(np.float32)\n",
    "\n",
    "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    bin_ids = np.digitize(conf, bins[1:-1], right=True)\n",
    "\n",
    "    bin_acc = np.zeros(n_bins, dtype=np.float32)\n",
    "    bin_conf = np.zeros(n_bins, dtype=np.float32)\n",
    "    bin_frac = np.zeros(n_bins, dtype=np.float32)\n",
    "\n",
    "    for b in range(n_bins):\n",
    "        mask = bin_ids == b\n",
    "        if mask.any():\n",
    "            bin_acc[b] = correct[mask].mean()\n",
    "            bin_conf[b] = conf[mask].mean()\n",
    "            bin_frac[b] = mask.mean()\n",
    "\n",
    "    ece = np.sum(np.abs(bin_acc - bin_conf) * bin_frac)\n",
    "\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "    bin_width = bins[1] - bins[0]\n",
    "\n",
    "    plt.figure(figsize=(4.5, 4.5))\n",
    "    plt.plot([0, 1], [0, 1], '--', color='gray', linewidth=3)\n",
    "    plt.bar(bin_centers, bin_acc, width=bin_width, color='blue', edgecolor='black', label='Outputs')\n",
    "    gap = bin_conf - bin_acc\n",
    "    plt.bar(bin_centers, gap, bottom=bin_acc, width=bin_width, color='salmon', edgecolor='red',\n",
    "            alpha=0.4, hatch='//', label='Gap')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.text(0.95, 0.05, f'ECE={ece*100:.2f}', ha='right', va='bottom',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightsteelblue', alpha=0.8))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=600)\n",
    "\n",
    "plot_reliability_diagram(lda_model, test_ld, 'plots/cifar10_lda_reliability_diagram.png', title='LDA')\n",
    "plot_reliability_diagram(softmax_model, test_ld, 'plots/cifar10_softmax_reliability_diagram.png', title='Softmax')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be52016b",
   "metadata": {},
   "source": [
    "### PCA over Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d812f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_embeddings(model, loader, batches=10):\n",
    "    model.eval()\n",
    "    embeds, labels = [], []\n",
    "    for i, (x, y) in enumerate(loader):   # use train_ld if you prefer\n",
    "        x = x.to(device)\n",
    "        z = model.encoder(x).cpu()\n",
    "        embeds.append(z)\n",
    "        labels.append(y)\n",
    "        if i >= batches - 1:   # 10 batches â‰ˆ10k points; raise/lower to taste\n",
    "            break\n",
    "\n",
    "    z = torch.cat(embeds)\n",
    "    y = torch.cat(labels)\n",
    "\n",
    "    # 2D projection (PCA)\n",
    "    z0 = z - z.mean(0, keepdim=True)\n",
    "    _, _, V = torch.pca_lowrank(z0, q=2)\n",
    "    z2 = z0 @ V[:, :2]\n",
    "    return z2, y\n",
    "\n",
    "lda_z2, lda_y = collect_embeddings(lda_model, train_ld)\n",
    "softmax_z2, softmax_y = collect_embeddings(softmax_model, train_ld)\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 14, \"axes.labelsize\": 16, \"legend.fontsize\": 13,\n",
    "    \"xtick.labelsize\": 13, \"ytick.labelsize\": 13\n",
    "})\n",
    "\n",
    "palette = plt.cm.tab10.colors\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "plots = [\n",
    "    (axes[0], lda_z2, lda_y, \"Deep LDA embeddings\"),\n",
    "    (axes[1], softmax_z2, softmax_y, \"Softmax head embeddings\"),\n",
    "]\n",
    "\n",
    "for ax, z2, y, title in plots:\n",
    "    for c in range(10):\n",
    "        idx = y == c\n",
    "        ax.scatter(z2[idx, 0], z2[idx, 1], s=8, alpha=0.6, color=palette[c], label=train_ds.classes[c])\n",
    "    ax.set_xlabel(\"PC1\")\n",
    "    ax.set_ylabel(\"PC2\")\n",
    "    ax.set_title(title)\n",
    "\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, bbox_to_anchor=(0.5, -0.02), loc=\"lower center\", ncol=5)\n",
    "plt.tight_layout(rect=(0, 0.07, 1, 1))\n",
    "plt.savefig('plots/cifar10_lda_vs_softmax_embeddings.png', dpi=600)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0802afa3",
   "metadata": {},
   "source": [
    "# CIFAR-100 Classification with an LDA Head\n",
    "This notebook trains a lightweight convolutional encoder with a linear discriminant analysis (LDA) head on CIFAR-100, then visualises the learned embedding space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c1fc5d",
   "metadata": {},
   "source": [
    "### Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf11f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from dnll import LDAHead, DNLLLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab011bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device =', device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7881674",
   "metadata": {},
   "source": [
    "### Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8e436e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = (0.5071, 0.4867, 0.4408)\n",
    "std = (0.2675, 0.2565, 0.2761)\n",
    "pin_memory = torch.cuda.is_available()\n",
    "\n",
    "train_tfm = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "test_tfm = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "train_ds = datasets.CIFAR100(root='./data', train=True, transform=train_tfm, download=True)\n",
    "test_ds  = datasets.CIFAR100(root='./data', train=False, transform=test_tfm, download=True)\n",
    "train_ld = DataLoader(train_ds, batch_size=256, shuffle=True, num_workers=4, pin_memory=pin_memory)\n",
    "test_ld  = DataLoader(test_ds,  batch_size=1024, shuffle=False, num_workers=4, pin_memory=pin_memory)\n",
    "len(train_ds), len(test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0efcdea",
   "metadata": {},
   "source": [
    "### Model: encoder + LDA head (on-the-fly stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daebb93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "        )\n",
    "        self.proj = nn.Linear(256, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.proj(x)\n",
    "\n",
    "class DeepLDA(nn.Module):\n",
    "    def __init__(self, C, D):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(D)\n",
    "        self.head = LDAHead(C, D)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.head(z)\n",
    "\n",
    "class SoftmaxHead(nn.Module):\n",
    "    def __init__(self, D, C):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(D, C)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.linear(z)\n",
    "\n",
    "class DeepClassifier(nn.Module):\n",
    "    def __init__(self, C, D):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(D)\n",
    "        self.head = SoftmaxHead(D, C)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.head(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd8e3cd",
   "metadata": {},
   "source": [
    "### Train & Eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f80cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    ok = tot = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        ok += (logits.argmax(1) == y).sum().item()\n",
    "        tot += y.size(0)\n",
    "    return ok / tot\n",
    "\n",
    "model = DeepLDA(C=100, D=99).to(device)\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "#loss_fn = nn.CrossEntropyLoss()\n",
    "#loss_fn = nn.NLLLoss()\n",
    "#loss_fn = LDALoss()\n",
    "#loss_fn = nn.PoissonNLLLoss()\n",
    "#loss_fn = LogisticLoss()\n",
    "loss_fn = DNLLLoss(lambda_reg=.01)\n",
    "\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    model.train()\n",
    "    loss_sum = acc_sum = n_sum = 0\n",
    "    for x, y in train_ld:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        #y_oh = 100 * F.one_hot(y, num_classes=100).to(logits.dtype)  # shape [batch,100]\n",
    "        loss = loss_fn(logits, y)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        with torch.no_grad():\n",
    "            pred = logits.argmax(1)\n",
    "            acc_sum += (pred == y).sum().item()\n",
    "            n_sum += y.size(0)\n",
    "            loss_sum += loss.item() * y.size(0)\n",
    "    tr_acc = acc_sum / n_sum\n",
    "    te_acc = evaluate(model, test_ld)\n",
    "    train_acc.append(tr_acc)\n",
    "    test_acc.append(te_acc)\n",
    "    print(f\"[{epoch:02d}] train loss={loss_sum/n_sum:.4f} acc={tr_acc:.4f} | test acc={te_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax head baseline\n",
    "Train a second model with the same encoder but a standard softmax classifier for a side-by-side comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_model = DeepClassifier(C=100, D=99).to(device)\n",
    "softmax_opt = torch.optim.Adam(softmax_model.parameters())\n",
    "softmax_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "softmax_train_acc = []\n",
    "softmax_test_acc = []\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    softmax_model.train()\n",
    "    loss_sum = acc_sum = n_sum = 0\n",
    "    for x, y in train_ld:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = softmax_model(x)\n",
    "        loss = softmax_loss_fn(logits, y)\n",
    "        softmax_opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        softmax_opt.step()\n",
    "        with torch.no_grad():\n",
    "            pred = logits.argmax(1)\n",
    "            acc_sum += (pred == y).sum().item()\n",
    "            n_sum += y.size(0)\n",
    "            loss_sum += loss.item() * y.size(0)\n",
    "    tr_acc = acc_sum / n_sum\n",
    "    te_acc = evaluate(softmax_model, test_ld)\n",
    "    softmax_train_acc.append(tr_acc)\n",
    "    softmax_test_acc.append(te_acc)\n",
    "    print(f\"[Softmax {epoch:02d}] train loss={loss_sum/n_sum:.4f} acc={tr_acc:.4f} | test acc={te_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f4c333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "conf_list = []\n",
    "pred_list = []\n",
    "label_list = []\n",
    "with torch.no_grad():\n",
    "    for x, y in test_ld:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        conf, pred = probs.max(1)\n",
    "        conf_list.append(conf.cpu())\n",
    "        pred_list.append(pred.cpu())\n",
    "        label_list.append(y.cpu())\n",
    "\n",
    "conf = torch.cat(conf_list).numpy()\n",
    "pred = torch.cat(pred_list)\n",
    "labels = torch.cat(label_list)\n",
    "acc = (pred == labels).float().mean().item()\n",
    "avg_conf = conf.mean().item()\n",
    "\n",
    "bins = np.linspace(0.0, 1.0, 21)\n",
    "weights = np.ones_like(conf) / conf.size\n",
    "\n",
    "plt.figure(figsize=(4.5, 4.5))\n",
    "plt.hist(conf, bins=bins, weights=weights, color='blue', edgecolor='black')\n",
    "plt.axvline(avg_conf, color='gray', linestyle='--', linewidth=3)\n",
    "plt.axvline(acc, color='gray', linestyle='--', linewidth=3)\n",
    "plt.text(avg_conf, 0.95 * plt.gca().get_ylim()[1], 'Avg confidence',\n",
    "         rotation=90, va='top', ha='center')\n",
    "plt.text(acc, 0.95 * plt.gca().get_ylim()[1], 'Accuracy',\n",
    "         rotation=90, va='top', ha='center')\n",
    "plt.xlabel('Confidence')\n",
    "plt.ylabel('% of Samples')\n",
    "plt.xlim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/cifar100_confidence_hist.png', dpi=600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013d4195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "conf_list = []\n",
    "pred_list = []\n",
    "label_list = []\n",
    "with torch.no_grad():\n",
    "    for x, y in test_ld:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        conf, pred = probs.max(1)\n",
    "        conf_list.append(conf.cpu())\n",
    "        pred_list.append(pred.cpu())\n",
    "        label_list.append(y.cpu())\n",
    "\n",
    "conf = torch.cat(conf_list).numpy()\n",
    "pred = torch.cat(pred_list).numpy()\n",
    "labels = torch.cat(label_list).numpy()\n",
    "correct = (pred == labels).astype(np.float32)\n",
    "\n",
    "n_bins = 10\n",
    "bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "bin_ids = np.digitize(conf, bins[1:-1], right=True)\n",
    "\n",
    "bin_acc = np.zeros(n_bins, dtype=np.float32)\n",
    "bin_conf = np.zeros(n_bins, dtype=np.float32)\n",
    "bin_frac = np.zeros(n_bins, dtype=np.float32)\n",
    "\n",
    "for b in range(n_bins):\n",
    "    mask = bin_ids == b\n",
    "    if mask.any():\n",
    "        bin_acc[b] = correct[mask].mean()\n",
    "        bin_conf[b] = conf[mask].mean()\n",
    "        bin_frac[b] = mask.mean()\n",
    "\n",
    "ece = np.sum(np.abs(bin_acc - bin_conf) * bin_frac)\n",
    "\n",
    "bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "bin_width = bins[1] - bins[0]\n",
    "\n",
    "plt.figure(figsize=(4.5, 4.5))\n",
    "plt.plot([0, 1], [0, 1], '--', color='gray', linewidth=3)\n",
    "plt.bar(bin_centers, bin_acc, width=bin_width, color='blue', edgecolor='black', label='Outputs')\n",
    "gap = bin_conf - bin_acc\n",
    "plt.bar(bin_centers, gap, bottom=bin_acc, width=bin_width, color='salmon', edgecolor='red',\n",
    "        alpha=0.4, hatch='//', label='Gap')\n",
    "plt.xlabel('Confidence')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(loc='upper left')\n",
    "plt.text(0.95, 0.05, f'ECE={ece*100:.2f}', ha='right', va='bottom',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightsteelblue', alpha=0.8))\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/cifar100_reliability_diagram.png', dpi=600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff1de92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, len(train_acc) + 1)\n",
    "plt.figure(figsize=(6, 4))\n",
    "#plt.plot(epochs, train_acc, label='Train accuracy')\n",
    "plt.plot(epochs, test_acc, label='Test accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('CIFAR-100 accuracy during training')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff97720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "save_path = 'cifar100_model.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': opt.state_dict(),\n",
    "}, save_path)\n",
    "print(f'Saved model checkpoint to {save_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA over Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_embeddings(model, loader, batches=10):\n",
    "    model.eval()\n",
    "    embeds, labels = [], []\n",
    "    for i, (x, y) in enumerate(loader):   # use train_ld if you prefer\n",
    "        x = x.to(device)\n",
    "        z = model.encoder(x).cpu()\n",
    "        embeds.append(z)\n",
    "        labels.append(y)\n",
    "        if i >= batches - 1:   # 10 batches ~10k points; raise/lower to taste\n",
    "            break\n",
    "\n",
    "    z = torch.cat(embeds)\n",
    "    y = torch.cat(labels)\n",
    "\n",
    "    # 2D projection (PCA)\n",
    "    z0 = z - z.mean(0, keepdim=True)\n",
    "    _, _, V = torch.pca_lowrank(z0, q=2)\n",
    "    z2 = z0 @ V[:, :2]\n",
    "    return z2, y\n",
    "\n",
    "lda_model = model\n",
    "if 'softmax_model' not in globals():\n",
    "    raise RuntimeError('softmax_model not found. Train or load a Softmax baseline before running this cell.')\n",
    "lda_z2, lda_y = collect_embeddings(lda_model, train_ld)\n",
    "softmax_z2, softmax_y = collect_embeddings(softmax_model, train_ld)\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 14, \"axes.labelsize\": 16, \"legend.fontsize\": 13,\n",
    "    \"xtick.labelsize\": 13, \"ytick.labelsize\": 13\n",
    "})\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "plots = [\n",
    "    (axes[0], lda_z2, lda_y, \"Deep LDA embeddings\"),\n",
    "    (axes[1], softmax_z2, softmax_y, \"Softmax head embeddings\"),\n",
    "]\n",
    "\n",
    "for ax, z2, y, title in plots:\n",
    "    sc = ax.scatter(z2[:, 0], z2[:, 1], c=y, cmap=\"nipy_spectral\",\n",
    "                    s=8, alpha=0.6, vmin=0, vmax=99, rasterized=True)\n",
    "    ax.set_xlabel(\"PC1\")\n",
    "    ax.set_ylabel(\"PC2\")\n",
    "    ax.set_title(title)\n",
    "\n",
    "cbar = fig.colorbar(sc, ax=axes, fraction=0.046, pad=0.04)\n",
    "cbar.set_label(\"Class index\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/cifar100_lda_vs_softmax_embeddings.png', dpi=600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea49e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a few examples from the two closest classes\n",
    "import numpy as np\n",
    "\n",
    "# Find closest pair using class means from the previous cell\n",
    "with torch.no_grad():\n",
    "    dist_mat = torch.cdist(class_means, class_means)\n",
    "    dist_mat.fill_diagonal_(float('inf'))\n",
    "    row_min, row_argmin = dist_mat.min(dim=1)\n",
    "    cls_a = row_min.argmin().item()\n",
    "    cls_b = row_argmin[cls_a].item()\n",
    "\n",
    "cls_names = train_ds.classes\n",
    "print(f\"Closest classes: {cls_a} ({cls_names[cls_a]}) and {cls_b} ({cls_names[cls_b]}), distance={dist_mat[cls_a, cls_b]:.4f}\")\n",
    "\n",
    "# Collect a few raw images (no augmentation) for each class from the training set\n",
    "n_per_class = 4\n",
    "indices_a = [i for i, t in enumerate(train_ds.targets) if t == cls_a][:n_per_class]\n",
    "indices_b = [i for i, t in enumerate(train_ds.targets) if t == cls_b][:n_per_class]\n",
    "\n",
    "fig, axes = plt.subplots(2, n_per_class, figsize=(2 * n_per_class, 4))\n",
    "for j, idx in enumerate(indices_a):\n",
    "    axes[0, j].imshow(train_ds.data[idx])\n",
    "    axes[0, j].axis('off')\n",
    "    axes[0, j].set_title(f\"{cls_names[cls_a]}\")\n",
    "for j, idx in enumerate(indices_b):\n",
    "    axes[1, j].imshow(train_ds.data[idx])\n",
    "    axes[1, j].axis('off')\n",
    "    axes[1, j].set_title(f\"{cls_names[cls_b]}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc8716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show examples from the 2nd closest pair of classes\n",
    "# Assumes class_means computed above\n",
    "with torch.no_grad():\n",
    "    dist_mat = torch.cdist(class_means, class_means)\n",
    "    dist_mat.fill_diagonal_(float('inf'))\n",
    "    i_idx, j_idx = torch.triu_indices(dist_mat.size(0), dist_mat.size(1), offset=1)\n",
    "    pair_dists = dist_mat[i_idx, j_idx]\n",
    "    if pair_dists.numel() < 2:\n",
    "        raise RuntimeError('Need at least two class pairs to find the 2nd closest pair.')\n",
    "    sorted_vals, sorted_idx = torch.sort(pair_dists)\n",
    "    second_idx = sorted_idx[1].item()\n",
    "    cls_a = i_idx[second_idx].item()\n",
    "    cls_b = j_idx[second_idx].item()\n",
    "    pair_dist = sorted_vals[1].item()\n",
    "\n",
    "cls_names = train_ds.classes\n",
    "print(f\"2nd closest classes: {cls_a} ({cls_names[cls_a]}) and {cls_b} ({cls_names[cls_b]}), distance={pair_dist:.4f}\")\n",
    "\n",
    "n_per_class = 4\n",
    "indices_a = [i for i, t in enumerate(train_ds.targets) if t == cls_a][:n_per_class]\n",
    "indices_b = [i for i, t in enumerate(train_ds.targets) if t == cls_b][:n_per_class]\n",
    "\n",
    "fig, axes = plt.subplots(2, n_per_class, figsize=(2 * n_per_class, 4))\n",
    "for j, idx in enumerate(indices_a):\n",
    "    axes[0, j].imshow(train_ds.data[idx])\n",
    "    axes[0, j].axis('off')\n",
    "    axes[0, j].set_title(f\"{cls_names[cls_a]}\")\n",
    "for j, idx in enumerate(indices_b):\n",
    "    axes[1, j].imshow(train_ds.data[idx])\n",
    "    axes[1, j].axis('off')\n",
    "    axes[1, j].set_title(f\"{cls_names[cls_b]}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09d6f7f",
   "metadata": {},
   "source": [
    "# Fashion-MNIST Classification with an LDA Head\n",
    "This notebook trains a small convolutional encoder with a linear discriminant analysis (LDA) head on Fashion-MNIST, then visualises the learned embedding space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f89626",
   "metadata": {},
   "source": [
    "### Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80764dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from dnll import LDAHead, DNLLLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd667682",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device =', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc74c579",
   "metadata": {},
   "source": [
    "### Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbdd16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm = transforms.ToTensor()\n",
    "train_ds = datasets.FashionMNIST(root='./data', train=True, transform=tfm, download=True)\n",
    "test_ds  = datasets.FashionMNIST(root='./data', train=False, transform=tfm, download=True)\n",
    "train_ld = DataLoader(train_ds, batch_size=256, shuffle=True, num_workers=2, pin_memory=True)\n",
    "test_ld  = DataLoader(test_ds,  batch_size=1024, shuffle=False, num_workers=2, pin_memory=True)\n",
    "len(train_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaab3fc",
   "metadata": {},
   "source": [
    "### Model: encoder + LDA head (on-the-fly stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a641b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "        )\n",
    "        self.proj = nn.Linear(256, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.proj(x)\n",
    "\n",
    "class DeepLDA(nn.Module):\n",
    "    def __init__(self, C, D):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(D)\n",
    "        self.head = LDAHead(C, D)\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.head(z)\n",
    "\n",
    "class SoftmaxHead(nn.Module):\n",
    "    def __init__(self, D, C):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(D, C)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.linear(z)\n",
    "\n",
    "class DeepClassifier(nn.Module):\n",
    "    def __init__(self, C, D):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(D)\n",
    "        self.head = SoftmaxHead(D, C)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.head(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bc29e0",
   "metadata": {},
   "source": [
    "### Train & Eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8039b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    ok = tot = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)  # EMA stats\n",
    "        ok += (logits.argmax(1) == y).sum().item()\n",
    "        tot += y.size(0)\n",
    "    return ok / tot\n",
    "\n",
    "model = DeepLDA(C=10, D=9).to(device)\n",
    "opt = torch.optim.Adam(model.encoder.parameters())\n",
    "#loss_fn = nn.CrossEntropyLoss()\n",
    "#loss_fn = nn.NLLLoss()\n",
    "#loss_fn = LDALoss()\n",
    "loss_fn = DNLLLoss(lambda_reg=.01)\n",
    "#loss_fn = LogisticLoss()\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    model.train()\n",
    "    loss_sum = acc_sum = n_sum = 0\n",
    "    for x, y in train_ld:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)        # uses batch stats + EMA update\n",
    "        loss = loss_fn(logits, y)\n",
    "        opt.zero_grad(set_to_none=True); loss.backward(); opt.step()\n",
    "        with torch.no_grad():\n",
    "            pred = logits.argmax(1)\n",
    "            acc_sum += (pred == y).sum().item()\n",
    "            n_sum += y.size(0)\n",
    "            loss_sum += loss.item() * y.size(0)\n",
    "    tr_acc = acc_sum / n_sum\n",
    "    te_acc = evaluate(model, test_ld)\n",
    "    print(f\"[{epoch:02d}] train loss={loss_sum/n_sum:.4f} acc={tr_acc:.4f} | test acc={te_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff47189b",
   "metadata": {},
   "source": [
    "### Softmax head baseline\n",
    "Train a second model with the same encoder but a standard softmax classifier for a side-by-side comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493d1935",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_model = DeepClassifier(C=10, D=9).to(device)\n",
    "softmax_opt = torch.optim.Adam(softmax_model.parameters())\n",
    "softmax_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "softmax_train_acc = []\n",
    "softmax_test_acc = []\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    softmax_model.train()\n",
    "    loss_sum = acc_sum = n_sum = 0\n",
    "    for x, y in train_ld:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = softmax_model(x)\n",
    "        loss = softmax_loss_fn(logits, y)\n",
    "        softmax_opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        softmax_opt.step()\n",
    "        with torch.no_grad():\n",
    "            pred = logits.argmax(1)\n",
    "            acc_sum += (pred == y).sum().item()\n",
    "            n_sum += y.size(0)\n",
    "            loss_sum += loss.item() * y.size(0)\n",
    "    tr_acc = acc_sum / n_sum\n",
    "    te_acc = evaluate(softmax_model, test_ld)\n",
    "    softmax_train_acc.append(tr_acc)\n",
    "    softmax_test_acc.append(te_acc)\n",
    "    print(f\"[Softmax {epoch:02d}] train loss={loss_sum/n_sum:.4f} acc={tr_acc:.4f} | test acc={te_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e642bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@torch.no_grad()\n",
    "def plot_confidence_hist(model, loader, out_path, title=None):\n",
    "    model.eval()\n",
    "    conf_list, pred_list, label_list = [], [], []\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        conf, pred = probs.max(1)\n",
    "        conf_list.append(conf.cpu())\n",
    "        pred_list.append(pred.cpu())\n",
    "        label_list.append(y.cpu())\n",
    "\n",
    "    conf = torch.cat(conf_list).numpy()\n",
    "    pred = torch.cat(pred_list)\n",
    "    labels = torch.cat(label_list)\n",
    "    acc = (pred == labels).float().mean().item()\n",
    "    avg_conf = conf.mean().item()\n",
    "\n",
    "    bins = np.linspace(0.0, 1.0, 21)\n",
    "    weights = np.ones_like(conf) / conf.size\n",
    "\n",
    "    plt.figure(figsize=(4.5, 4.5))\n",
    "    plt.hist(conf, bins=bins, weights=weights, color='blue', edgecolor='black')\n",
    "    plt.axvline(avg_conf, color='gray', linestyle='--', linewidth=3)\n",
    "    plt.axvline(acc, color='gray', linestyle='--', linewidth=3)\n",
    "    plt.text(avg_conf, 0.95 * plt.gca().get_ylim()[1], 'Avg confidence',\n",
    "             rotation=90, va='top', ha='center')\n",
    "    plt.text(acc, 0.95 * plt.gca().get_ylim()[1], 'Accuracy',\n",
    "             rotation=90, va='top', ha='center')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('% of Samples')\n",
    "    plt.xlim(0, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=600)\n",
    "\n",
    "plot_confidence_hist(model, test_ld, 'plots/fashion_mnist_lda_confidence_hist.png', title='LDA')\n",
    "plot_confidence_hist(softmax_model, test_ld, 'plots/fashion_mnist_softmax_confidence_hist.png', title='Softmax')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb936cc0",
   "metadata": {},
   "source": [
    "### Reliability diagram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9235af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@torch.no_grad()\n",
    "def plot_reliability_diagram(model, loader, out_path, title=None, n_bins=10):\n",
    "    model.eval()\n",
    "    conf_list, pred_list, label_list = [], [], []\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        conf, pred = probs.max(1)\n",
    "        conf_list.append(conf.cpu())\n",
    "        pred_list.append(pred.cpu())\n",
    "        label_list.append(y.cpu())\n",
    "\n",
    "    conf = torch.cat(conf_list).numpy()\n",
    "    pred = torch.cat(pred_list).numpy()\n",
    "    labels = torch.cat(label_list).numpy()\n",
    "    correct = (pred == labels).astype(np.float32)\n",
    "\n",
    "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    bin_ids = np.digitize(conf, bins[1:-1], right=True)\n",
    "\n",
    "    bin_acc = np.zeros(n_bins, dtype=np.float32)\n",
    "    bin_conf = np.zeros(n_bins, dtype=np.float32)\n",
    "    bin_frac = np.zeros(n_bins, dtype=np.float32)\n",
    "\n",
    "    for b in range(n_bins):\n",
    "        mask = bin_ids == b\n",
    "        if mask.any():\n",
    "            bin_acc[b] = correct[mask].mean()\n",
    "            bin_conf[b] = conf[mask].mean()\n",
    "            bin_frac[b] = mask.mean()\n",
    "\n",
    "    ece = np.sum(np.abs(bin_acc - bin_conf) * bin_frac)\n",
    "\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "    bin_width = bins[1] - bins[0]\n",
    "\n",
    "    plt.figure(figsize=(4.5, 4.5))\n",
    "    plt.plot([0, 1], [0, 1], '--', color='gray', linewidth=3)\n",
    "    plt.bar(bin_centers, bin_acc, width=bin_width, color='blue', edgecolor='black', label='Outputs')\n",
    "    gap = bin_conf - bin_acc\n",
    "    plt.bar(bin_centers, gap, bottom=bin_acc, width=bin_width, color='salmon', edgecolor='red',\n",
    "            alpha=0.4, hatch='//', label='Gap')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.text(0.95, 0.05, f'ECE={ece*100:.2f}', ha='right', va='bottom',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightsteelblue', alpha=0.8))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=600)\n",
    "\n",
    "plot_reliability_diagram(model, test_ld, 'plots/fashion_mnist_lda_reliability_diagram.png', title='LDA')\n",
    "plot_reliability_diagram(softmax_model, test_ld, 'plots/fashion_mnist_softmax_reliability_diagram.png', title='Softmax')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f289ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_embeddings(model, loader, batches=10):\n",
    "    model.eval()\n",
    "    embeds, labels = [], []\n",
    "    for i, (x, y) in enumerate(loader):   # use train_ld if you prefer\n",
    "        x = x.to(device)\n",
    "        z = model.encoder(x).cpu()\n",
    "        embeds.append(z)\n",
    "        labels.append(y)\n",
    "        if i >= batches - 1:   # 10 batches ~10k points; raise/lower to taste\n",
    "            break\n",
    "\n",
    "    z = torch.cat(embeds)\n",
    "    y = torch.cat(labels)\n",
    "\n",
    "    # 2D projection (PCA)\n",
    "    z0 = z - z.mean(0, keepdim=True)\n",
    "    _, _, V = torch.pca_lowrank(z0, q=2)\n",
    "    z2 = z0 @ V[:, :2]\n",
    "    return z2, y\n",
    "\n",
    "lda_model = model\n",
    "if 'softmax_model' not in globals():\n",
    "    raise RuntimeError('softmax_model not found. Train or load a Softmax baseline before running this cell.')\n",
    "lda_z2, lda_y = collect_embeddings(lda_model, train_ld)\n",
    "softmax_z2, softmax_y = collect_embeddings(softmax_model, train_ld)\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 14, \"axes.labelsize\": 16, \"legend.fontsize\": 13,\n",
    "    \"xtick.labelsize\": 13, \"ytick.labelsize\": 13\n",
    "})\n",
    "\n",
    "palette = plt.cm.tab10.colors\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "plots = [\n",
    "    (axes[1], lda_z2, lda_y, \"LDA\"),\n",
    "    (axes[0], softmax_z2, softmax_y, \"Softmax\"),\n",
    "]\n",
    "\n",
    "for ax, z2, y, title in plots:\n",
    "    for c in range(10):\n",
    "        idx = y == c\n",
    "        ax.scatter(z2[idx, 0], z2[idx, 1], s=8, alpha=0.6, color=palette[c], label=train_ds.classes[c])\n",
    "    ax.set_xlabel(\"PC1\")\n",
    "    ax.set_ylabel(\"PC2\")\n",
    "    ax.set_title(title)\n",
    "\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, bbox_to_anchor=(0.5, -0.02), loc=\"lower center\", ncol=5)\n",
    "plt.tight_layout(rect=(0, 0.07, 1, 1))\n",
    "plt.savefig('plots/fashion_mnist_lda_vs_softmax_embeddings.png', dpi=600)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dzz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
